{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyFobH-4dW2J","outputId":"d722b140-b81f-4e42-f07b-72e268ce8ccf","executionInfo":{"status":"ok","timestamp":1731818556869,"user_tz":-330,"elapsed":14901,"user":{"displayName":"adil sidan","userId":"12920459609366235558"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dadaptation\n","  Downloading dadaptation-3.2.tar.gz (13 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: dadaptation\n","  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dadaptation: filename=dadaptation-3.2-py3-none-any.whl size=23208 sha256=a4b2dff9699f130e05c1577cdbab0eff6ae3f4932508581acf244c90188e1177\n","  Stored in directory: /root/.cache/pip/wheels/d0/03/6d/feba04df15ef39d9ac4e3504058ac2a88fb2ef9183ba92b111\n","Successfully built dadaptation\n","Installing collected packages: dadaptation\n","Successfully installed dadaptation-3.2\n"]}],"source":["!pip install dadaptation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KT9qz9gpz97E"},"outputs":[],"source":["from dadaptation.dadapt_adagrad import DAdaptAdaGrad\n","from dadaptation.dadapt_adam import DAdaptAdam\n","from dadaptation.dadapt_sgd import DAdaptSGD\n","from dadaptation.dadapt_adan import DAdaptAdan\n","from dadaptation.dadapt_lion import DAdaptLion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMHgW1EhcBlI"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","\n","\n","\n","# Wide Residual Block definition\n","class WideResNetBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, drop_rate=0.0):\n","        super(WideResNetBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.drop_rate = drop_rate\n","\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n","        else:\n","            self.shortcut = nn.Identity()\n","\n","    def forward(self, x):\n","        out = self.conv1(F.relu(self.bn1(x)))\n","        if self.drop_rate > 0:\n","            out = F.dropout(out, p=self.drop_rate, training=self.training)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += self.shortcut(x)\n","        return out\n","\n","# WideResNet model definition\n","class WideResNet(nn.Module):\n","    def __init__(self, depth, widen_factor, num_classes, drop_rate=0.0):\n","        super(WideResNet, self).__init__()\n","        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n","        n = (depth - 4) // 6\n","        k = widen_factor\n","\n","        # Initial convolution\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","        # WideResNet blocks\n","        self.block1 = self._make_layer(16, 16 * k, n, stride=1, drop_rate=drop_rate)\n","        self.block2 = self._make_layer(16 * k, 32 * k, n, stride=2, drop_rate=drop_rate)\n","        self.block3 = self._make_layer(32 * k, 64 * k, n, stride=2, drop_rate=drop_rate)\n","\n","        # Batch normalization, linear layer, and global average pooling\n","        self.bn1 = nn.BatchNorm2d(64 * k)\n","        self.fc = nn.Linear(64 * k, num_classes)\n","\n","        # Initialize weights\n","        self._initialize_weights()\n","\n","    def _make_layer(self, in_channels, out_channels, num_blocks, stride, drop_rate):\n","        layers = []\n","        for i in range(num_blocks):\n","            layers.append(WideResNetBlock(\n","                in_channels=in_channels if i == 0 else out_channels,\n","                out_channels=out_channels,\n","                stride=stride if i == 0 else 1,\n","                drop_rate=drop_rate\n","            ))\n","        return nn.Sequential(*layers)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.block1(out)\n","        out = self.block2(out)\n","        out = self.block3(out)\n","        out = F.relu(self.bn1(out))\n","        out = F.avg_pool2d(out, 8)\n","        out = out.view(out.size(0), -1)\n","        return self.fc(out)\n","\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import math\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Prepare the CIFAR-10 dataset and DataLoader\n","transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.ToTensor(),\n","])\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","# Initialize model, criterion, and optimizers\n","model = WideResNet(depth=10, widen_factor=4, num_classes=10).to(device)  # WRN-16-8 for CIFAR-10\n","criterion = nn.CrossEntropyLoss()\n","\n","# Save the initial model state for reinitialization\n","initial_state_dict = model.state_dict()\n","\n","\n","\n","# Function to train with specific d0 values\n","def train_with_d0_cifar(d0, epochs=200):\n","    # Reinitialize the model to start fresh for each d0\n","    model.load_state_dict(initial_state_dict)  # Reset to the initial saved state\n","    optimizer = DAdaptSGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4, d0=d0)\n","\n","    train_losses = []\n","    train_accs = []\n","    scaler = torch.cuda.amp.GradScaler()  # Mixed precision scaling\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            # Enable mixed precision\n","            with torch.cuda.amp.autocast():\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        train_acc = 100 * correct / total\n","\n","        train_losses.append(avg_loss)\n","        train_accs.append(train_acc)\n","\n","        # Print training progress\n","        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.2f}%')\n","\n","    return train_losses, train_accs\n","\n","d0_values = [1.0, 100]\n","all_train_losses = {}\n","all_train_accs = {}\n","\n","for d0 in d0_values:\n","    print(f'\\nTraining with d0 = {d0}')\n","    train_losses, train_accs = train_with_d0_cifar(d0)\n","    all_train_losses[d0] = train_losses\n","    all_train_accs[d0] = train_accs\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_jxXugRT-2J","outputId":"0307ecf6-8fe1-4a2f-d0b3-8fa06f9d4399","executionInfo":{"status":"ok","timestamp":1731829066829,"user_tz":-330,"elapsed":474102,"user":{"displayName":"adil sidan","userId":"12920459609366235558"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","\n","Training with d0 = 1.0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-5-0c03f5e16b38>:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()  # Mixed precision scaling\n","<ipython-input-5-0c03f5e16b38>:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/200], Loss: 2.1871, Training Accuracy: 28.08%\n","Epoch [2/200], Loss: 1.7167, Training Accuracy: 37.98%\n","Epoch [3/200], Loss: 1.6067, Training Accuracy: 41.85%\n","Epoch [4/200], Loss: 1.5303, Training Accuracy: 45.13%\n","Epoch [5/200], Loss: 1.4669, Training Accuracy: 47.13%\n","Epoch [6/200], Loss: 1.4137, Training Accuracy: 49.29%\n","Epoch [7/200], Loss: 1.3676, Training Accuracy: 50.99%\n","Epoch [8/200], Loss: 1.3306, Training Accuracy: 52.27%\n","Epoch [9/200], Loss: 1.2957, Training Accuracy: 53.71%\n","Epoch [10/200], Loss: 1.2623, Training Accuracy: 55.15%\n","Epoch [11/200], Loss: 1.2374, Training Accuracy: 55.97%\n","Epoch [12/200], Loss: 1.2116, Training Accuracy: 57.09%\n","Epoch [13/200], Loss: 1.1838, Training Accuracy: 58.13%\n","Epoch [14/200], Loss: 1.1619, Training Accuracy: 58.81%\n","Epoch [15/200], Loss: 1.1367, Training Accuracy: 59.79%\n","Epoch [16/200], Loss: 1.1116, Training Accuracy: 60.88%\n","Epoch [17/200], Loss: 1.0911, Training Accuracy: 61.61%\n","Epoch [18/200], Loss: 1.0761, Training Accuracy: 62.29%\n","Epoch [19/200], Loss: 1.0519, Training Accuracy: 63.10%\n","Epoch [20/200], Loss: 1.0370, Training Accuracy: 63.50%\n","Epoch [21/200], Loss: 1.0167, Training Accuracy: 64.48%\n","Epoch [22/200], Loss: 1.0009, Training Accuracy: 64.97%\n","Epoch [23/200], Loss: 0.9830, Training Accuracy: 65.47%\n","Epoch [24/200], Loss: 0.9673, Training Accuracy: 66.05%\n","Epoch [25/200], Loss: 0.9501, Training Accuracy: 66.66%\n","Epoch [26/200], Loss: 0.9341, Training Accuracy: 67.37%\n","Epoch [27/200], Loss: 0.9201, Training Accuracy: 67.72%\n","Epoch [28/200], Loss: 0.9047, Training Accuracy: 68.32%\n","Epoch [29/200], Loss: 0.8885, Training Accuracy: 69.14%\n","Epoch [30/200], Loss: 0.8775, Training Accuracy: 69.35%\n","Epoch [31/200], Loss: 0.8612, Training Accuracy: 69.99%\n","Epoch [32/200], Loss: 0.8463, Training Accuracy: 70.77%\n","Epoch [33/200], Loss: 0.8335, Training Accuracy: 70.86%\n","Epoch [34/200], Loss: 0.8228, Training Accuracy: 71.20%\n","Epoch [35/200], Loss: 0.8046, Training Accuracy: 72.07%\n","Epoch [36/200], Loss: 0.7918, Training Accuracy: 72.33%\n","Epoch [37/200], Loss: 0.7788, Training Accuracy: 72.68%\n","Epoch [38/200], Loss: 0.7691, Training Accuracy: 73.36%\n","Epoch [39/200], Loss: 0.7586, Training Accuracy: 73.51%\n","Epoch [40/200], Loss: 0.7409, Training Accuracy: 74.19%\n","Epoch [41/200], Loss: 0.7334, Training Accuracy: 74.68%\n","Epoch [42/200], Loss: 0.7197, Training Accuracy: 74.98%\n","Epoch [43/200], Loss: 0.7090, Training Accuracy: 75.52%\n","Epoch [44/200], Loss: 0.7004, Training Accuracy: 75.86%\n","Epoch [45/200], Loss: 0.6870, Training Accuracy: 76.33%\n","Epoch [46/200], Loss: 0.6800, Training Accuracy: 76.67%\n","Epoch [47/200], Loss: 0.6698, Training Accuracy: 76.79%\n","Epoch [48/200], Loss: 0.6598, Training Accuracy: 77.24%\n","Epoch [49/200], Loss: 0.6488, Training Accuracy: 77.62%\n","Epoch [50/200], Loss: 0.6411, Training Accuracy: 77.55%\n","Epoch [51/200], Loss: 0.6305, Training Accuracy: 78.22%\n","Epoch [52/200], Loss: 0.6206, Training Accuracy: 78.65%\n","Epoch [53/200], Loss: 0.6153, Training Accuracy: 78.85%\n","Epoch [54/200], Loss: 0.6057, Training Accuracy: 79.21%\n","Epoch [55/200], Loss: 0.5968, Training Accuracy: 79.47%\n","Epoch [56/200], Loss: 0.5877, Training Accuracy: 79.67%\n","Epoch [57/200], Loss: 0.5817, Training Accuracy: 80.05%\n","Epoch [58/200], Loss: 0.5738, Training Accuracy: 80.14%\n","Epoch [59/200], Loss: 0.5674, Training Accuracy: 80.41%\n","Epoch [60/200], Loss: 0.5550, Training Accuracy: 80.85%\n","Epoch [61/200], Loss: 0.5536, Training Accuracy: 81.04%\n","Epoch [62/200], Loss: 0.5406, Training Accuracy: 81.48%\n","Epoch [63/200], Loss: 0.5383, Training Accuracy: 81.54%\n","Epoch [64/200], Loss: 0.5295, Training Accuracy: 81.76%\n","Epoch [65/200], Loss: 0.5264, Training Accuracy: 81.64%\n","Epoch [66/200], Loss: 0.5178, Training Accuracy: 82.14%\n","Epoch [67/200], Loss: 0.5123, Training Accuracy: 82.57%\n","Epoch [68/200], Loss: 0.5034, Training Accuracy: 82.77%\n","Epoch [69/200], Loss: 0.4996, Training Accuracy: 82.77%\n","Epoch [70/200], Loss: 0.4907, Training Accuracy: 83.14%\n","Epoch [71/200], Loss: 0.4860, Training Accuracy: 83.39%\n","Epoch [72/200], Loss: 0.4797, Training Accuracy: 83.50%\n","Epoch [73/200], Loss: 0.4740, Training Accuracy: 83.60%\n","Epoch [74/200], Loss: 0.4691, Training Accuracy: 83.92%\n","Epoch [75/200], Loss: 0.4615, Training Accuracy: 84.15%\n","Epoch [76/200], Loss: 0.4547, Training Accuracy: 84.38%\n","Epoch [77/200], Loss: 0.4533, Training Accuracy: 84.54%\n","Epoch [78/200], Loss: 0.4462, Training Accuracy: 84.68%\n","Epoch [79/200], Loss: 0.4413, Training Accuracy: 84.95%\n","Epoch [80/200], Loss: 0.4348, Training Accuracy: 85.12%\n","Epoch [81/200], Loss: 0.4311, Training Accuracy: 85.15%\n","Epoch [82/200], Loss: 0.4278, Training Accuracy: 85.35%\n","Epoch [83/200], Loss: 0.4212, Training Accuracy: 85.63%\n","Epoch [84/200], Loss: 0.4210, Training Accuracy: 85.56%\n","Epoch [85/200], Loss: 0.4098, Training Accuracy: 85.95%\n","Epoch [86/200], Loss: 0.4083, Training Accuracy: 86.18%\n","Epoch [87/200], Loss: 0.4021, Training Accuracy: 86.32%\n","Epoch [88/200], Loss: 0.3993, Training Accuracy: 86.40%\n","Epoch [89/200], Loss: 0.3943, Training Accuracy: 86.59%\n","Epoch [90/200], Loss: 0.3883, Training Accuracy: 86.67%\n","Epoch [91/200], Loss: 0.3842, Training Accuracy: 86.89%\n","Epoch [92/200], Loss: 0.3767, Training Accuracy: 87.13%\n","Epoch [93/200], Loss: 0.3799, Training Accuracy: 86.97%\n","Epoch [94/200], Loss: 0.3737, Training Accuracy: 87.25%\n","Epoch [95/200], Loss: 0.3723, Training Accuracy: 87.27%\n","Epoch [96/200], Loss: 0.3679, Training Accuracy: 87.55%\n","Epoch [97/200], Loss: 0.3624, Training Accuracy: 87.78%\n","Epoch [98/200], Loss: 0.3573, Training Accuracy: 87.74%\n","Epoch [99/200], Loss: 0.3548, Training Accuracy: 87.77%\n","Epoch [100/200], Loss: 0.3460, Training Accuracy: 88.19%\n","Epoch [101/200], Loss: 0.3475, Training Accuracy: 88.08%\n","Epoch [102/200], Loss: 0.3463, Training Accuracy: 88.13%\n","Epoch [103/200], Loss: 0.3418, Training Accuracy: 88.30%\n","Epoch [104/200], Loss: 0.3356, Training Accuracy: 88.43%\n","Epoch [105/200], Loss: 0.3316, Training Accuracy: 88.71%\n","Epoch [106/200], Loss: 0.3244, Training Accuracy: 89.02%\n","Epoch [107/200], Loss: 0.3238, Training Accuracy: 88.92%\n","Epoch [108/200], Loss: 0.3221, Training Accuracy: 89.00%\n","Epoch [109/200], Loss: 0.3187, Training Accuracy: 88.98%\n","Epoch [110/200], Loss: 0.3134, Training Accuracy: 89.28%\n","Epoch [111/200], Loss: 0.3125, Training Accuracy: 89.28%\n","Epoch [112/200], Loss: 0.3059, Training Accuracy: 89.60%\n","Epoch [113/200], Loss: 0.3038, Training Accuracy: 89.58%\n","Epoch [114/200], Loss: 0.3044, Training Accuracy: 89.47%\n","Epoch [115/200], Loss: 0.2956, Training Accuracy: 89.89%\n","Epoch [116/200], Loss: 0.2954, Training Accuracy: 89.88%\n","Epoch [117/200], Loss: 0.2935, Training Accuracy: 90.02%\n","Epoch [118/200], Loss: 0.2881, Training Accuracy: 90.15%\n","Epoch [119/200], Loss: 0.2892, Training Accuracy: 90.18%\n","Epoch [120/200], Loss: 0.2826, Training Accuracy: 90.31%\n","Epoch [121/200], Loss: 0.2790, Training Accuracy: 90.28%\n","Epoch [122/200], Loss: 0.2750, Training Accuracy: 90.56%\n","Epoch [123/200], Loss: 0.2784, Training Accuracy: 90.58%\n","Epoch [124/200], Loss: 0.2704, Training Accuracy: 90.72%\n","Epoch [125/200], Loss: 0.2722, Training Accuracy: 90.64%\n","Epoch [126/200], Loss: 0.2645, Training Accuracy: 90.94%\n","Epoch [127/200], Loss: 0.2606, Training Accuracy: 91.08%\n","Epoch [128/200], Loss: 0.2605, Training Accuracy: 91.10%\n","Epoch [129/200], Loss: 0.2587, Training Accuracy: 91.21%\n","Epoch [130/200], Loss: 0.2565, Training Accuracy: 91.23%\n","Epoch [131/200], Loss: 0.2538, Training Accuracy: 91.31%\n","Epoch [132/200], Loss: 0.2527, Training Accuracy: 91.21%\n","Epoch [133/200], Loss: 0.2494, Training Accuracy: 91.36%\n","Epoch [134/200], Loss: 0.2435, Training Accuracy: 91.70%\n","Epoch [135/200], Loss: 0.2403, Training Accuracy: 91.83%\n","Epoch [136/200], Loss: 0.2447, Training Accuracy: 91.63%\n","Epoch [137/200], Loss: 0.2351, Training Accuracy: 91.87%\n","Epoch [138/200], Loss: 0.2345, Training Accuracy: 91.97%\n","Epoch [139/200], Loss: 0.2312, Training Accuracy: 92.12%\n","Epoch [140/200], Loss: 0.2295, Training Accuracy: 92.01%\n","Epoch [141/200], Loss: 0.2285, Training Accuracy: 92.21%\n","Epoch [142/200], Loss: 0.2247, Training Accuracy: 92.47%\n","Epoch [143/200], Loss: 0.2267, Training Accuracy: 92.25%\n","Epoch [144/200], Loss: 0.2205, Training Accuracy: 92.54%\n","Epoch [145/200], Loss: 0.2194, Training Accuracy: 92.41%\n","Epoch [146/200], Loss: 0.2167, Training Accuracy: 92.59%\n","Epoch [147/200], Loss: 0.2136, Training Accuracy: 92.80%\n","Epoch [148/200], Loss: 0.2133, Training Accuracy: 92.76%\n","Epoch [149/200], Loss: 0.2094, Training Accuracy: 92.72%\n","Epoch [150/200], Loss: 0.2084, Training Accuracy: 93.00%\n","Epoch [151/200], Loss: 0.2054, Training Accuracy: 93.00%\n","Epoch [152/200], Loss: 0.2011, Training Accuracy: 93.26%\n","Epoch [153/200], Loss: 0.2014, Training Accuracy: 93.09%\n","Epoch [154/200], Loss: 0.1997, Training Accuracy: 93.19%\n","Epoch [155/200], Loss: 0.2003, Training Accuracy: 93.21%\n","Epoch [156/200], Loss: 0.1943, Training Accuracy: 93.46%\n","Epoch [157/200], Loss: 0.1919, Training Accuracy: 93.56%\n","Epoch [158/200], Loss: 0.1947, Training Accuracy: 93.41%\n","Epoch [159/200], Loss: 0.1883, Training Accuracy: 93.63%\n","Epoch [160/200], Loss: 0.1907, Training Accuracy: 93.53%\n","Epoch [161/200], Loss: 0.1844, Training Accuracy: 93.71%\n","Epoch [162/200], Loss: 0.1886, Training Accuracy: 93.58%\n","Epoch [163/200], Loss: 0.1832, Training Accuracy: 93.67%\n","Epoch [164/200], Loss: 0.1827, Training Accuracy: 93.81%\n","Epoch [165/200], Loss: 0.1810, Training Accuracy: 93.85%\n","Epoch [166/200], Loss: 0.1831, Training Accuracy: 93.76%\n","Epoch [167/200], Loss: 0.1717, Training Accuracy: 94.23%\n","Epoch [168/200], Loss: 0.1753, Training Accuracy: 94.10%\n","Epoch [169/200], Loss: 0.1700, Training Accuracy: 94.22%\n","Epoch [170/200], Loss: 0.1681, Training Accuracy: 94.27%\n","Epoch [171/200], Loss: 0.1687, Training Accuracy: 94.15%\n","Epoch [172/200], Loss: 0.1677, Training Accuracy: 94.26%\n","Epoch [173/200], Loss: 0.1670, Training Accuracy: 94.33%\n","Epoch [174/200], Loss: 0.1645, Training Accuracy: 94.41%\n","Epoch [175/200], Loss: 0.1635, Training Accuracy: 94.49%\n","Epoch [176/200], Loss: 0.1636, Training Accuracy: 94.54%\n","Epoch [177/200], Loss: 0.1599, Training Accuracy: 94.52%\n","Epoch [178/200], Loss: 0.1601, Training Accuracy: 94.56%\n","Epoch [179/200], Loss: 0.1617, Training Accuracy: 94.47%\n","Epoch [180/200], Loss: 0.1548, Training Accuracy: 94.65%\n","Epoch [181/200], Loss: 0.1530, Training Accuracy: 94.80%\n","Epoch [182/200], Loss: 0.1502, Training Accuracy: 95.11%\n","Epoch [183/200], Loss: 0.1506, Training Accuracy: 94.87%\n","Epoch [184/200], Loss: 0.1548, Training Accuracy: 94.75%\n","Epoch [185/200], Loss: 0.1467, Training Accuracy: 95.04%\n","Epoch [186/200], Loss: 0.1519, Training Accuracy: 94.88%\n","Epoch [187/200], Loss: 0.1481, Training Accuracy: 95.10%\n","Epoch [188/200], Loss: 0.1494, Training Accuracy: 94.86%\n","Epoch [189/200], Loss: 0.1450, Training Accuracy: 95.11%\n","Epoch [190/200], Loss: 0.1408, Training Accuracy: 95.41%\n","Epoch [191/200], Loss: 0.1423, Training Accuracy: 95.19%\n","Epoch [192/200], Loss: 0.1420, Training Accuracy: 95.27%\n","Epoch [193/200], Loss: 0.1396, Training Accuracy: 95.37%\n","Epoch [194/200], Loss: 0.1365, Training Accuracy: 95.47%\n","Epoch [195/200], Loss: 0.1412, Training Accuracy: 95.28%\n","Epoch [196/200], Loss: 0.1356, Training Accuracy: 95.56%\n","Epoch [197/200], Loss: 0.1353, Training Accuracy: 95.35%\n","Epoch [198/200], Loss: 0.1385, Training Accuracy: 95.40%\n","Epoch [199/200], Loss: 0.1339, Training Accuracy: 95.52%\n","Epoch [200/200], Loss: 0.1343, Training Accuracy: 95.47%\n","\n","Training with d0 = 100\n","Epoch [1/200], Loss: 0.8279, Training Accuracy: 71.96%\n","Epoch [2/200], Loss: 0.5401, Training Accuracy: 81.33%\n","Epoch [3/200], Loss: 0.4749, Training Accuracy: 83.71%\n","Epoch [4/200], Loss: 0.4403, Training Accuracy: 84.97%\n","Epoch [5/200], Loss: 0.4203, Training Accuracy: 85.52%\n","Epoch [6/200], Loss: 0.3998, Training Accuracy: 86.36%\n","Epoch [7/200], Loss: 0.3905, Training Accuracy: 86.54%\n","Epoch [8/200], Loss: 0.3792, Training Accuracy: 86.99%\n","Epoch [9/200], Loss: 0.3690, Training Accuracy: 87.55%\n","Epoch [10/200], Loss: 0.3607, Training Accuracy: 87.58%\n","Epoch [11/200], Loss: 0.3504, Training Accuracy: 88.12%\n","Epoch [12/200], Loss: 0.3475, Training Accuracy: 88.09%\n","Epoch [13/200], Loss: 0.3470, Training Accuracy: 88.15%\n","Epoch [14/200], Loss: 0.3401, Training Accuracy: 88.36%\n","Epoch [15/200], Loss: 0.3348, Training Accuracy: 88.62%\n","Epoch [16/200], Loss: 0.3333, Training Accuracy: 88.47%\n","Epoch [17/200], Loss: 0.3278, Training Accuracy: 88.85%\n","Epoch [18/200], Loss: 0.3230, Training Accuracy: 88.95%\n","Epoch [19/200], Loss: 0.3239, Training Accuracy: 88.87%\n","Epoch [20/200], Loss: 0.3189, Training Accuracy: 89.07%\n","Epoch [21/200], Loss: 0.3193, Training Accuracy: 88.96%\n","Epoch [22/200], Loss: 0.3169, Training Accuracy: 89.22%\n","Epoch [23/200], Loss: 0.3129, Training Accuracy: 89.27%\n","Epoch [24/200], Loss: 0.3087, Training Accuracy: 89.56%\n","Epoch [25/200], Loss: 0.3056, Training Accuracy: 89.46%\n","Epoch [26/200], Loss: 0.3037, Training Accuracy: 89.67%\n","Epoch [27/200], Loss: 0.3041, Training Accuracy: 89.63%\n","Epoch [28/200], Loss: 0.3002, Training Accuracy: 89.65%\n","Epoch [29/200], Loss: 0.2992, Training Accuracy: 89.73%\n","Epoch [30/200], Loss: 0.3005, Training Accuracy: 89.70%\n","Epoch [31/200], Loss: 0.2959, Training Accuracy: 89.83%\n","Epoch [32/200], Loss: 0.2960, Training Accuracy: 89.89%\n","Epoch [33/200], Loss: 0.2912, Training Accuracy: 89.80%\n","Epoch [34/200], Loss: 0.2952, Training Accuracy: 89.90%\n","Epoch [35/200], Loss: 0.2921, Training Accuracy: 89.92%\n","Epoch [36/200], Loss: 0.2855, Training Accuracy: 90.21%\n","Epoch [37/200], Loss: 0.2875, Training Accuracy: 90.11%\n","Epoch [38/200], Loss: 0.2845, Training Accuracy: 90.18%\n","Epoch [39/200], Loss: 0.2859, Training Accuracy: 90.10%\n","Epoch [40/200], Loss: 0.2885, Training Accuracy: 89.97%\n","Epoch [41/200], Loss: 0.2827, Training Accuracy: 90.34%\n","Epoch [42/200], Loss: 0.2853, Training Accuracy: 90.17%\n","Epoch [43/200], Loss: 0.2828, Training Accuracy: 90.43%\n","Epoch [44/200], Loss: 0.2806, Training Accuracy: 90.29%\n","Epoch [45/200], Loss: 0.2805, Training Accuracy: 90.48%\n","Epoch [46/200], Loss: 0.2838, Training Accuracy: 90.30%\n","Epoch [47/200], Loss: 0.2843, Training Accuracy: 90.24%\n","Epoch [48/200], Loss: 0.2734, Training Accuracy: 90.83%\n","Epoch [49/200], Loss: 0.2764, Training Accuracy: 90.63%\n","Epoch [50/200], Loss: 0.2781, Training Accuracy: 90.45%\n","Epoch [51/200], Loss: 0.2759, Training Accuracy: 90.52%\n","Epoch [52/200], Loss: 0.2780, Training Accuracy: 90.46%\n","Epoch [53/200], Loss: 0.2745, Training Accuracy: 90.46%\n","Epoch [54/200], Loss: 0.2782, Training Accuracy: 90.46%\n","Epoch [55/200], Loss: 0.2791, Training Accuracy: 90.48%\n","Epoch [56/200], Loss: 0.2694, Training Accuracy: 90.79%\n","Epoch [57/200], Loss: 0.2732, Training Accuracy: 90.69%\n","Epoch [58/200], Loss: 0.2729, Training Accuracy: 90.63%\n","Epoch [59/200], Loss: 0.2720, Training Accuracy: 90.65%\n","Epoch [60/200], Loss: 0.2738, Training Accuracy: 90.60%\n","Epoch [61/200], Loss: 0.2712, Training Accuracy: 90.66%\n","Epoch [62/200], Loss: 0.2708, Training Accuracy: 90.71%\n","Epoch [63/200], Loss: 0.2682, Training Accuracy: 90.82%\n","Epoch [64/200], Loss: 0.2629, Training Accuracy: 91.02%\n","Epoch [65/200], Loss: 0.2761, Training Accuracy: 90.47%\n","Epoch [66/200], Loss: 0.2667, Training Accuracy: 90.80%\n","Epoch [67/200], Loss: 0.2659, Training Accuracy: 90.83%\n","Epoch [68/200], Loss: 0.2636, Training Accuracy: 90.89%\n","Epoch [69/200], Loss: 0.2642, Training Accuracy: 90.93%\n","Epoch [70/200], Loss: 0.2679, Training Accuracy: 90.77%\n","Epoch [71/200], Loss: 0.2657, Training Accuracy: 90.77%\n","Epoch [72/200], Loss: 0.2622, Training Accuracy: 91.15%\n","Epoch [73/200], Loss: 0.2655, Training Accuracy: 90.88%\n","Epoch [74/200], Loss: 0.2666, Training Accuracy: 90.86%\n","Epoch [75/200], Loss: 0.2653, Training Accuracy: 90.87%\n","Epoch [76/200], Loss: 0.2554, Training Accuracy: 91.19%\n","Epoch [77/200], Loss: 0.2574, Training Accuracy: 91.26%\n","Epoch [78/200], Loss: 0.2653, Training Accuracy: 91.08%\n","Epoch [79/200], Loss: 0.2607, Training Accuracy: 91.02%\n","Epoch [80/200], Loss: 0.2668, Training Accuracy: 90.87%\n","Epoch [81/200], Loss: 0.2560, Training Accuracy: 91.15%\n","Epoch [82/200], Loss: 0.2611, Training Accuracy: 91.10%\n","Epoch [83/200], Loss: 0.2576, Training Accuracy: 91.18%\n","Epoch [84/200], Loss: 0.2602, Training Accuracy: 91.09%\n","Epoch [85/200], Loss: 0.2624, Training Accuracy: 91.00%\n","Epoch [86/200], Loss: 0.2573, Training Accuracy: 91.13%\n","Epoch [87/200], Loss: 0.2607, Training Accuracy: 90.97%\n","Epoch [88/200], Loss: 0.2581, Training Accuracy: 91.00%\n","Epoch [89/200], Loss: 0.2561, Training Accuracy: 91.20%\n","Epoch [90/200], Loss: 0.2535, Training Accuracy: 91.31%\n","Epoch [91/200], Loss: 0.2545, Training Accuracy: 91.29%\n","Epoch [92/200], Loss: 0.2566, Training Accuracy: 91.31%\n","Epoch [93/200], Loss: 0.2567, Training Accuracy: 91.19%\n","Epoch [94/200], Loss: 0.2582, Training Accuracy: 91.10%\n","Epoch [95/200], Loss: 0.2582, Training Accuracy: 91.06%\n","Epoch [96/200], Loss: 0.2546, Training Accuracy: 91.34%\n","Epoch [97/200], Loss: 0.2538, Training Accuracy: 91.21%\n","Epoch [98/200], Loss: 0.2553, Training Accuracy: 91.39%\n","Epoch [99/200], Loss: 0.2495, Training Accuracy: 91.47%\n","Epoch [100/200], Loss: 0.2525, Training Accuracy: 91.44%\n","Epoch [101/200], Loss: 0.2543, Training Accuracy: 91.25%\n","Epoch [102/200], Loss: 0.2511, Training Accuracy: 91.39%\n","Epoch [103/200], Loss: 0.2541, Training Accuracy: 91.32%\n","Epoch [104/200], Loss: 0.2557, Training Accuracy: 91.23%\n","Epoch [105/200], Loss: 0.2552, Training Accuracy: 91.17%\n","Epoch [106/200], Loss: 0.2534, Training Accuracy: 91.33%\n","Epoch [107/200], Loss: 0.2567, Training Accuracy: 91.33%\n","Epoch [108/200], Loss: 0.2510, Training Accuracy: 91.39%\n","Epoch [109/200], Loss: 0.2546, Training Accuracy: 91.26%\n","Epoch [110/200], Loss: 0.2534, Training Accuracy: 91.24%\n","Epoch [111/200], Loss: 0.2531, Training Accuracy: 91.32%\n","Epoch [112/200], Loss: 0.2518, Training Accuracy: 91.46%\n","Epoch [113/200], Loss: 0.2495, Training Accuracy: 91.34%\n","Epoch [114/200], Loss: 0.2483, Training Accuracy: 91.54%\n","Epoch [115/200], Loss: 0.2501, Training Accuracy: 91.49%\n","Epoch [116/200], Loss: 0.2509, Training Accuracy: 91.45%\n","Epoch [117/200], Loss: 0.2459, Training Accuracy: 91.63%\n","Epoch [118/200], Loss: 0.2495, Training Accuracy: 91.37%\n","Epoch [119/200], Loss: 0.2513, Training Accuracy: 91.22%\n","Epoch [120/200], Loss: 0.2524, Training Accuracy: 91.33%\n","Epoch [121/200], Loss: 0.2468, Training Accuracy: 91.57%\n","Epoch [122/200], Loss: 0.2476, Training Accuracy: 91.59%\n","Epoch [123/200], Loss: 0.2457, Training Accuracy: 91.59%\n","Epoch [124/200], Loss: 0.2488, Training Accuracy: 91.55%\n","Epoch [125/200], Loss: 0.2483, Training Accuracy: 91.57%\n","Epoch [126/200], Loss: 0.2471, Training Accuracy: 91.53%\n","Epoch [127/200], Loss: 0.2530, Training Accuracy: 91.38%\n","Epoch [128/200], Loss: 0.2441, Training Accuracy: 91.71%\n","Epoch [129/200], Loss: 0.2483, Training Accuracy: 91.51%\n","Epoch [130/200], Loss: 0.2494, Training Accuracy: 91.47%\n","Epoch [131/200], Loss: 0.2463, Training Accuracy: 91.59%\n","Epoch [132/200], Loss: 0.2528, Training Accuracy: 91.32%\n","Epoch [133/200], Loss: 0.2523, Training Accuracy: 91.29%\n","Epoch [134/200], Loss: 0.2447, Training Accuracy: 91.51%\n","Epoch [135/200], Loss: 0.2445, Training Accuracy: 91.53%\n","Epoch [136/200], Loss: 0.2514, Training Accuracy: 91.51%\n","Epoch [137/200], Loss: 0.2480, Training Accuracy: 91.54%\n","Epoch [138/200], Loss: 0.2467, Training Accuracy: 91.57%\n","Epoch [139/200], Loss: 0.2428, Training Accuracy: 91.65%\n","Epoch [140/200], Loss: 0.2440, Training Accuracy: 91.59%\n","Epoch [141/200], Loss: 0.2486, Training Accuracy: 91.49%\n","Epoch [142/200], Loss: 0.2405, Training Accuracy: 91.89%\n","Epoch [143/200], Loss: 0.2440, Training Accuracy: 91.77%\n","Epoch [144/200], Loss: 0.2469, Training Accuracy: 91.67%\n","Epoch [145/200], Loss: 0.2463, Training Accuracy: 91.61%\n","Epoch [146/200], Loss: 0.2451, Training Accuracy: 91.62%\n","Epoch [147/200], Loss: 0.2436, Training Accuracy: 91.57%\n","Epoch [148/200], Loss: 0.2469, Training Accuracy: 91.47%\n","Epoch [149/200], Loss: 0.2427, Training Accuracy: 91.71%\n","Epoch [150/200], Loss: 0.2470, Training Accuracy: 91.53%\n","Epoch [151/200], Loss: 0.2388, Training Accuracy: 91.87%\n","Epoch [152/200], Loss: 0.2477, Training Accuracy: 91.41%\n","Epoch [153/200], Loss: 0.2436, Training Accuracy: 91.56%\n","Epoch [154/200], Loss: 0.2411, Training Accuracy: 91.76%\n","Epoch [155/200], Loss: 0.2421, Training Accuracy: 91.75%\n","Epoch [156/200], Loss: 0.2415, Training Accuracy: 91.88%\n","Epoch [157/200], Loss: 0.2400, Training Accuracy: 91.79%\n","Epoch [158/200], Loss: 0.2451, Training Accuracy: 91.56%\n","Epoch [159/200], Loss: 0.2434, Training Accuracy: 91.76%\n","Epoch [160/200], Loss: 0.2452, Training Accuracy: 91.56%\n","Epoch [161/200], Loss: 0.2421, Training Accuracy: 91.64%\n","Epoch [162/200], Loss: 0.2442, Training Accuracy: 91.67%\n","Epoch [163/200], Loss: 0.2403, Training Accuracy: 91.78%\n","Epoch [164/200], Loss: 0.2453, Training Accuracy: 91.62%\n","Epoch [165/200], Loss: 0.2416, Training Accuracy: 91.81%\n","Epoch [166/200], Loss: 0.2449, Training Accuracy: 91.60%\n","Epoch [167/200], Loss: 0.2410, Training Accuracy: 91.72%\n","Epoch [168/200], Loss: 0.2471, Training Accuracy: 91.52%\n","Epoch [169/200], Loss: 0.2421, Training Accuracy: 91.74%\n","Epoch [170/200], Loss: 0.2411, Training Accuracy: 91.67%\n","Epoch [171/200], Loss: 0.2401, Training Accuracy: 91.83%\n","Epoch [172/200], Loss: 0.2443, Training Accuracy: 91.67%\n","Epoch [173/200], Loss: 0.2407, Training Accuracy: 91.71%\n","Epoch [174/200], Loss: 0.2399, Training Accuracy: 91.83%\n","Epoch [175/200], Loss: 0.2443, Training Accuracy: 91.53%\n","Epoch [176/200], Loss: 0.2368, Training Accuracy: 91.89%\n","Epoch [177/200], Loss: 0.2389, Training Accuracy: 91.88%\n","Epoch [178/200], Loss: 0.2415, Training Accuracy: 91.77%\n","Epoch [179/200], Loss: 0.2411, Training Accuracy: 91.85%\n","Epoch [180/200], Loss: 0.2484, Training Accuracy: 91.41%\n","Epoch [181/200], Loss: 0.2414, Training Accuracy: 91.79%\n","Epoch [182/200], Loss: 0.2374, Training Accuracy: 91.92%\n","Epoch [183/200], Loss: 0.2422, Training Accuracy: 91.73%\n","Epoch [184/200], Loss: 0.2450, Training Accuracy: 91.76%\n","Epoch [185/200], Loss: 0.2435, Training Accuracy: 91.73%\n","Epoch [186/200], Loss: 0.2371, Training Accuracy: 91.88%\n","Epoch [187/200], Loss: 0.2380, Training Accuracy: 91.88%\n","Epoch [188/200], Loss: 0.2338, Training Accuracy: 91.96%\n","Epoch [189/200], Loss: 0.2470, Training Accuracy: 91.60%\n","Epoch [190/200], Loss: 0.2406, Training Accuracy: 91.71%\n","Epoch [191/200], Loss: 0.2405, Training Accuracy: 91.77%\n","Epoch [192/200], Loss: 0.2450, Training Accuracy: 91.62%\n","Epoch [193/200], Loss: 0.2376, Training Accuracy: 91.67%\n","Epoch [194/200], Loss: 0.2440, Training Accuracy: 91.71%\n","Epoch [195/200], Loss: 0.2364, Training Accuracy: 91.94%\n","Epoch [196/200], Loss: 0.2417, Training Accuracy: 91.61%\n","Epoch [197/200], Loss: 0.2418, Training Accuracy: 91.66%\n","Epoch [198/200], Loss: 0.2442, Training Accuracy: 91.54%\n","Epoch [199/200], Loss: 0.2408, Training Accuracy: 91.63%\n","Epoch [200/200], Loss: 0.2379, Training Accuracy: 91.82%\n"]}]},{"cell_type":"code","source":["# Reinitialize model to initial state before training with second optimizer\n","model.load_state_dict(initial_state_dict)\n","\n","# Define the second optimizer with CyclicLR\n","base_lr = 0.01\n","max_lr = 0.1\n","optimizer_cyclic = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.CyclicLR(optimizer_cyclic, base_lr=base_lr, max_lr=max_lr, step_size_up=5, mode='triangular')\n","\n","# Train with SGD + CyclicLR using mixed precision\n","train_losses_cyclic, train_accs_cyclic = [], []\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer_cyclic.zero_grad()\n","\n","        # Mixed Precision Training\n","        with torch.amp.autocast('cuda', enabled=True):  # Enable mixed precision\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","        scaler.scale(loss).backward()  # Scaled backward pass\n","        scaler.step(optimizer_cyclic)  # Update model parameters\n","        scaler.update()  # Update the scale factor\n","        scheduler.step()  # Update the learning rate scheduler\n","\n","        total_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    train_acc = 100 * correct / total\n","    train_losses_cyclic.append(avg_loss)\n","    train_accs_cyclic.append(train_acc)\n","\n","    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.2f}%')"],"metadata":{"id":"qybFVZvIZBaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Plotting Loss and Accuracy Comparison\n","plt.figure(figsize=(14, 6))\n","\n","# Loss Comparison\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses_dadapt, label=f'DAdaptSGD', color='blue')\n","plt.plot(train_losses_cyclic, label=f'SGD + CyclicLR', color='orange')\n","plt.title('Training Loss Comparison (CIFAR-10)', fontsize=16)\n","plt.xlabel('Epochs', fontsize=14)\n","plt.ylabel('Loss', fontsize=14)\n","plt.legend(fontsize=12)\n","\n","# Accuracy Comparison\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accs_dadapt, label=f'DAdaptSGD', color='blue')\n","plt.plot(train_accs_cyclic, label=f'SGD + CyclicLR', color='orange')\n","plt.title('Training Accuracy Comparison (CIFAR-10)', fontsize=16)\n","plt.xlabel('Epochs', fontsize=14)\n","plt.ylabel('Accuracy (%)', fontsize=14)\n","plt.legend(fontsize=12)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"vGmxy00NTnjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kZP99GtHUlxS"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}